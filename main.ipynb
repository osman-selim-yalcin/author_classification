{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79817510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Her yazarın toplam kelime sayısı:\n",
      "                                                   text    author  label  \\\n",
      "0     Talihsiz Bedevi İle Kutup Ayısı\\n\\nBAŞBAKAN Er...    AHakan      2   \n",
      "1      Bir De Cool Olacaklar\\n\\nMİRGÜN’ün “eski sevg...    AHakan      2   \n",
      "2     İstihbarat Varmış\\n\\nDevlet, eskiden başörtüsü...    AHakan      2   \n",
      "3     Ateşten Gömlek: TÜSİAD Başkanlığı\\n\\n“Şu anda ...    AHakan      2   \n",
      "4     Antakya’da Sakallı Aradım\\n\\nSaat: 24.00... An...    AHakan      2   \n",
      "...                                                 ...       ...    ...   \n",
      "1195  Eski Hataları Tekrarlamayalım...\\n\\nBaşbakan Y...  MABirand     17   \n",
      "1196  Org. Özel, TSK'nın Eski Faturalarını Ödüyor......  MABirand     17   \n",
      "1197  PKK'nın Gövde Gösterisi Başarılı Ancak...\\n\\nG...  MABirand     17   \n",
      "1198  İsrail, İran'ı Vurunca Bizim De Başımız Çok De...  MABirand     17   \n",
      "1199  Hepimiz Kaybettik...\\n\\nUçakta önüme konan gaz...  MABirand     17   \n",
      "\n",
      "      word_count  \n",
      "0            913  \n",
      "1            849  \n",
      "2            813  \n",
      "3            661  \n",
      "4            994  \n",
      "...          ...  \n",
      "1195         565  \n",
      "1196         674  \n",
      "1197         641  \n",
      "1198         515  \n",
      "1199         436  \n",
      "\n",
      "[1200 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# Load documents assuming all are from 'aydın'\n",
    "def load_documents_from_folder(folder_path):\n",
    "    documents = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        author_files = folder_path + \"/\" + file_name\n",
    "        for txt_name in os.listdir(author_files):\n",
    "            if txt_name.endswith(\".txt\"):\n",
    "                with open(\n",
    "                    os.path.join(author_files, txt_name), \"r\", encoding=\"utf-8\"\n",
    "                ) as f:\n",
    "                    text = f.read()\n",
    "                    documents.append((text, file_name))\n",
    "    return documents\n",
    "\n",
    "\n",
    "folder_path = \"datas/\"\n",
    "data = load_documents_from_folder(folder_path)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"text\", \"author\"])\n",
    "le = LabelEncoder()\n",
    "df[\"label\"] = le.fit_transform(df[\"author\"])\n",
    "\n",
    "# Her yazının kelime sayısını hesapla\n",
    "df[\"word_count\"] = df[\"text\"].apply(lambda x: len(x.split()))\n",
    "author_word_counts = df.groupby(\"author\")[\"word_count\"].sum().sort_values(ascending=False)\n",
    "print(\"Her yazarın toplam kelime sayısı:\")\n",
    "print(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c6ea872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Her yazarın toplam kelime sayısı:\n",
      "author\n",
      "YCongar         58522\n",
      "CCandar         31918\n",
      "AHakan          31409\n",
      "RMengi          30415\n",
      "SOzisik         29242\n",
      "AAydintasbas    27580\n",
      "MABirand        26860\n",
      "DCundioglu      26235\n",
      "COzdemir        26208\n",
      "ATuranAlkan     24581\n",
      "PMagden         23978\n",
      "AAltan          23903\n",
      "MBaransu        23652\n",
      "NBKaraca        22920\n",
      "AYArslan        22653\n",
      "DUAribogan      22403\n",
      "ECakir          21210\n",
      "GGokturk        20967\n",
      "HCemal          20415\n",
      "MNHazar         19778\n",
      "HUluc           19525\n",
      "IKucukkaya      19312\n",
      "TAkyol          18445\n",
      "MTonbekici      17828\n",
      "EArdic          15746\n",
      "MBarlas         15309\n",
      "NIlicak         12763\n",
      "YOzdil          12157\n",
      "HBabaoglu       11271\n",
      "BCoskun          9107\n",
      "Name: word_count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\*+', ' ', text)         # *** -> boşluk\n",
    "    text = re.sub(r'\\n+', '\\n', text)        # çoklu satır boşluklarını teke indir\n",
    "    text = re.sub(r'[“”]', '\"', text)        # fancy tırnakları düzleştir\n",
    "    text = re.sub(r'[‘’]', \"'\", text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)      # fazla boşluğu tek boşluk yap\n",
    "    return text.strip()\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "df[\"word_count\"] = df[\"text\"].apply(lambda x: len(x.split()))\n",
    "author_word_counts = df.groupby(\"author\")[\"word_count\"].sum().sort_values(ascending=False)\n",
    "print(\"Her yazarın toplam kelime sayısı:\")\n",
    "print(author_word_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "556256d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifiers\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\"),\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"MLP\": MLPClassifier(max_iter=300),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "}\n",
    "\n",
    "# Define TF-IDF vectorizer settings\n",
    "vectorizer_settings = {\n",
    "    \"word_unigram\": TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 1)),\n",
    "    \"word_bigram_trigram\": TfidfVectorizer(analyzer=\"word\", ngram_range=(2, 3)),\n",
    "    \"char_bigram_trigram\": TfidfVectorizer(analyzer=\"char\", ngram_range=(2, 3)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac914d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Random Forest with word_unigram...\n",
      "Evaluating SVM with word_unigram...\n",
      "Evaluating XGBoost with word_unigram...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/osmanyalcin/Desktop/Osman selim yalçın/bert_env/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [11:17:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Naive Bayes with word_unigram...\n",
      "Evaluating MLP with word_unigram...\n",
      "Evaluating Decision Tree with word_unigram...\n",
      "Evaluating Random Forest with word_bigram_trigram...\n",
      "Evaluating SVM with word_bigram_trigram...\n",
      "Evaluating XGBoost with word_bigram_trigram...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/osmanyalcin/Desktop/Osman selim yalçın/bert_env/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [11:19:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Naive Bayes with word_bigram_trigram...\n",
      "Evaluating MLP with word_bigram_trigram...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/osmanyalcin/Desktop/Osman selim yalçın/bert_env/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Decision Tree with word_bigram_trigram...\n",
      "Evaluating Random Forest with char_bigram_trigram...\n",
      "Evaluating SVM with char_bigram_trigram...\n",
      "Evaluating XGBoost with char_bigram_trigram...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/osmanyalcin/Desktop/Osman selim yalçın/bert_env/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [11:25:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Naive Bayes with char_bigram_trigram...\n",
      "Evaluating MLP with char_bigram_trigram...\n",
      "Evaluating Decision Tree with char_bigram_trigram...\n",
      "TF-IDF Results:\n",
      "Feature: word_unigram, Model: Random Forest, Accuracy: 0.7083, Precision: 0.7775, Recall: 0.7083, F1-score: 0.7096\n",
      "Feature: word_unigram, Model: SVM, Accuracy: 0.4875, Precision: 0.7560, Recall: 0.4875, F1-score: 0.5082\n",
      "Feature: word_unigram, Model: XGBoost, Accuracy: 0.6042, Precision: 0.6174, Recall: 0.6042, F1-score: 0.5955\n",
      "Feature: word_unigram, Model: Naive Bayes, Accuracy: 0.3958, Precision: 0.6283, Recall: 0.3958, F1-score: 0.3917\n",
      "Feature: word_unigram, Model: MLP, Accuracy: 0.7750, Precision: 0.8100, Recall: 0.7750, F1-score: 0.7648\n",
      "Feature: word_unigram, Model: Decision Tree, Accuracy: 0.3167, Precision: 0.3400, Recall: 0.3167, F1-score: 0.3148\n",
      "Feature: word_bigram_trigram, Model: Random Forest, Accuracy: 0.3542, Precision: 0.5483, Recall: 0.3542, F1-score: 0.3736\n",
      "Feature: word_bigram_trigram, Model: SVM, Accuracy: 0.0167, Precision: 0.0293, Recall: 0.0167, F1-score: 0.0076\n",
      "Feature: word_bigram_trigram, Model: XGBoost, Accuracy: 0.2333, Precision: 0.3728, Recall: 0.2333, F1-score: 0.2512\n",
      "Feature: word_bigram_trigram, Model: Naive Bayes, Accuracy: 0.2125, Precision: 0.3832, Recall: 0.2125, F1-score: 0.2169\n",
      "Feature: word_bigram_trigram, Model: MLP, Accuracy: 0.0458, Precision: 0.0021, Recall: 0.0458, F1-score: 0.0040\n",
      "Feature: word_bigram_trigram, Model: Decision Tree, Accuracy: 0.2208, Precision: 0.2729, Recall: 0.2208, F1-score: 0.2292\n",
      "Feature: char_bigram_trigram, Model: Random Forest, Accuracy: 0.8500, Precision: 0.8693, Recall: 0.8500, F1-score: 0.8389\n",
      "Feature: char_bigram_trigram, Model: SVM, Accuracy: 0.9000, Precision: 0.9213, Recall: 0.9000, F1-score: 0.9012\n",
      "Feature: char_bigram_trigram, Model: XGBoost, Accuracy: 0.8833, Precision: 0.8973, Recall: 0.8833, F1-score: 0.8802\n",
      "Feature: char_bigram_trigram, Model: Naive Bayes, Accuracy: 0.1000, Precision: 0.0862, Recall: 0.1000, F1-score: 0.0600\n",
      "Feature: char_bigram_trigram, Model: MLP, Accuracy: 0.9208, Precision: 0.9281, Recall: 0.9208, F1-score: 0.9200\n",
      "Feature: char_bigram_trigram, Model: Decision Tree, Accuracy: 0.6458, Precision: 0.6656, Recall: 0.6458, F1-score: 0.6395\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# TF-IDF based evaluations\n",
    "for vec_name, vectorizer in vectorizer_settings.items():\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Evaluating {model_name} with {vec_name}...\")\n",
    "        pipeline = Pipeline([(\"tfidf\", vectorizer), (\"clf\", model)])\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        report = classification_report(\n",
    "            y_test, y_pred, output_dict=True, zero_division=0\n",
    "        )\n",
    "        results.append(\n",
    "            {\n",
    "                \"Feature\": vec_name,\n",
    "                \"Model\": model_name,\n",
    "                \"Accuracy\": report[\"accuracy\"],\n",
    "                \"Precision\": report[\"weighted avg\"][\"precision\"],\n",
    "                \"Recall\": report[\"weighted avg\"][\"recall\"],\n",
    "                \"F1-score\": report[\"weighted avg\"][\"f1-score\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(\"TF-IDF Results:\")\n",
    "for result in results:\n",
    "    print(\n",
    "        f\"Feature: {result['Feature']}, Model: {result['Model']}, \"\n",
    "        f\"Accuracy: {result['Accuracy']:.4f}, Precision: {result['Precision']:.4f}, \"\n",
    "        f\"Recall: {result['Recall']:.4f}, F1-score: {result['F1-score']:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8b05eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 30/30 [00:08<00:00,  3.39it/s]\n",
      "Batches: 100%|██████████| 8/8 [00:02<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Random Forest with BERT embeddings...\n",
      "Evaluating SVM with BERT embeddings...\n",
      "Evaluating XGBoost with BERT embeddings...\n",
      "Evaluating MLP with BERT embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/osmanyalcin/Desktop/Osman selim yalçın/bert_env/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Decision Tree with BERT embeddings...\n",
      "BERT Results:\n",
      "Feature: BERT, Model: Random Forest, Accuracy: 0.1375, Precision: 0.1600, Recall: 0.1375, F1-score: 0.1409\n",
      "Feature: BERT, Model: SVM, Accuracy: 0.1458, Precision: 0.1771, Recall: 0.1458, F1-score: 0.1284\n",
      "Feature: BERT, Model: XGBoost, Accuracy: 0.1375, Precision: 0.1327, Recall: 0.1375, F1-score: 0.1308\n",
      "Feature: BERT, Model: MLP, Accuracy: 0.1833, Precision: 0.1897, Recall: 0.1833, F1-score: 0.1801\n",
      "Feature: BERT, Model: Decision Tree, Accuracy: 0.0542, Precision: 0.0535, Recall: 0.0542, F1-score: 0.0526\n",
      "Results saved to results.csv\n"
     ]
    }
   ],
   "source": [
    "# BERT embeddings\n",
    "bert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "X_train_bert = bert_model.encode(X_train.tolist(), show_progress_bar=True)\n",
    "X_test_bert = bert_model.encode(X_test.tolist(), show_progress_bar=True)\n",
    "\n",
    "bert_compatible_models = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss'),\n",
    "    \"MLP\": MLPClassifier(max_iter=300),\n",
    "    \"Decision Tree\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "# TMP\n",
    "results = []\n",
    "\n",
    "for model_name, model in bert_compatible_models.items(): \n",
    "    print(f\"Evaluating {model_name} with BERT embeddings...\")\n",
    "    model.fit(X_train_bert, y_train)\n",
    "    y_pred = model.predict(X_test_bert)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "    results.append(\n",
    "        {\n",
    "            \"Feature\": \"BERT\",\n",
    "            \"Model\": model_name,\n",
    "            \"Accuracy\": report[\"accuracy\"],\n",
    "            \"Precision\": report[\"weighted avg\"][\"precision\"],\n",
    "            \"Recall\": report[\"weighted avg\"][\"recall\"],\n",
    "            \"F1-score\": report[\"weighted avg\"][\"f1-score\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"BERT Results:\")\n",
    "for result in results:\n",
    "    if result[\"Feature\"] == \"BERT\":\n",
    "        print(\n",
    "            f\"Feature: {result['Feature']}, Model: {result['Model']}, \"\n",
    "            f\"Accuracy: {result['Accuracy']:.4f}, Precision: {result['Precision']:.4f}, \"\n",
    "            f\"Recall: {result['Recall']:.4f}, F1-score: {result['F1-score']:.4f}\"\n",
    "        )\n",
    "# Save results to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"results2.csv\", index=False)\n",
    "print(\"Results saved to results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
