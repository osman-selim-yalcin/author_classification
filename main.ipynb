{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79817510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# Load documents \n",
    "def load_documents_from_folder(folder_path):\n",
    "    documents = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        author_files = folder_path + \"/\" + file_name\n",
    "        for txt_name in os.listdir(author_files):\n",
    "            if txt_name.endswith(\".txt\"):\n",
    "                with open(\n",
    "                    os.path.join(author_files, txt_name), \"r\", encoding=\"utf-8\"\n",
    "                ) as f:\n",
    "                    text = f.read()\n",
    "                    documents.append((text, file_name))\n",
    "    return documents\n",
    "\n",
    "\n",
    "folder_path = \"datas/\"\n",
    "data = load_documents_from_folder(folder_path)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"text\", \"author\"])\n",
    "le = LabelEncoder()\n",
    "df[\"label\"] = le.fit_transform(df[\"author\"])\n",
    "\n",
    "# Her yazının kelime sayısını hesapla\n",
    "df[\"word_count\"] = df[\"text\"].apply(lambda x: len(x.split()))\n",
    "author_word_counts = df.groupby(\"author\")[\"word_count\"].sum().sort_values(ascending=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c6ea872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Her yazarın toplam kelime sayısı:\n",
      "author\n",
      "YCongar         58522\n",
      "CCandar         31918\n",
      "AHakan          31409\n",
      "RMengi          30415\n",
      "SOzisik         29242\n",
      "AAydintasbas    27580\n",
      "MABirand        26860\n",
      "DCundioglu      26235\n",
      "COzdemir        26208\n",
      "ATuranAlkan     24581\n",
      "PMagden         23978\n",
      "AAltan          23903\n",
      "MBaransu        23652\n",
      "NBKaraca        22920\n",
      "AYArslan        22653\n",
      "DUAribogan      22403\n",
      "ECakir          21210\n",
      "GGokturk        20967\n",
      "HCemal          20415\n",
      "MNHazar         19778\n",
      "HUluc           19525\n",
      "IKucukkaya      19312\n",
      "TAkyol          18445\n",
      "MTonbekici      17828\n",
      "EArdic          15746\n",
      "MBarlas         15309\n",
      "NIlicak         12763\n",
      "YOzdil          12157\n",
      "HBabaoglu       11271\n",
      "BCoskun          9107\n",
      "Name: word_count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\*+', ' ', text)         # *** -> boşluk\n",
    "    text = re.sub(r'\\n+', '\\n', text)        # çoklu satır boşluklarını teke indir\n",
    "    text = re.sub(r'[“”]', '\"', text)        # fancy tırnakları düzleştir\n",
    "    text = re.sub(r'[‘’]', \"'\", text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)      # fazla boşluğu tek boşluk yap\n",
    "    return text.strip()\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "df[\"word_count\"] = df[\"text\"].apply(lambda x: len(x.split()))\n",
    "author_word_counts = df.groupby(\"author\")[\"word_count\"].sum().sort_values(ascending=False)\n",
    "print(\"Her yazarın toplam kelime sayısı:\")\n",
    "print(author_word_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "556256d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifiers\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\"),\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"MLP\": MLPClassifier(max_iter=300),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "}\n",
    "\n",
    "# Define TF-IDF vectorizer settings\n",
    "vectorizer_settings = {\n",
    "    \"word_unigram\": TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 1)),\n",
    "    \"word_bigram_trigram\": TfidfVectorizer(analyzer=\"word\", ngram_range=(2, 3)),\n",
    "    \"char_bigram_trigram\": TfidfVectorizer(analyzer=\"char\", ngram_range=(2, 3)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac914d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Random Forest with word_unigram...\n",
      "Evaluating SVM with word_unigram...\n",
      "Evaluating XGBoost with word_unigram...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/osmanyalcin/Desktop/ödevler/data mining/bert_env/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [09:59:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Naive Bayes with word_unigram...\n",
      "Evaluating MLP with word_unigram...\n",
      "Evaluating Decision Tree with word_unigram...\n",
      "Evaluating Random Forest with word_bigram_trigram...\n",
      "Evaluating SVM with word_bigram_trigram...\n",
      "Evaluating XGBoost with word_bigram_trigram...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/osmanyalcin/Desktop/ödevler/data mining/bert_env/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [10:00:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Naive Bayes with word_bigram_trigram...\n",
      "Evaluating MLP with word_bigram_trigram...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/osmanyalcin/Desktop/ödevler/data mining/bert_env/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Decision Tree with word_bigram_trigram...\n",
      "Evaluating Random Forest with char_bigram_trigram...\n",
      "Evaluating SVM with char_bigram_trigram...\n",
      "Evaluating XGBoost with char_bigram_trigram...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/osmanyalcin/Desktop/ödevler/data mining/bert_env/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [10:15:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Naive Bayes with char_bigram_trigram...\n",
      "Evaluating MLP with char_bigram_trigram...\n",
      "Evaluating Decision Tree with char_bigram_trigram...\n",
      "TF-IDF Results:\n",
      "Feature: word_unigram, Model: Random Forest, Accuracy: 0.6500, Precision: 0.6902, Recall: 0.6500, F1-score: 0.6369\n",
      "Feature: word_unigram, Model: SVM, Accuracy: 0.4875, Precision: 0.7560, Recall: 0.4875, F1-score: 0.5082\n",
      "Feature: word_unigram, Model: XGBoost, Accuracy: 0.6042, Precision: 0.6174, Recall: 0.6042, F1-score: 0.5955\n",
      "Feature: word_unigram, Model: Naive Bayes, Accuracy: 0.3958, Precision: 0.6283, Recall: 0.3958, F1-score: 0.3917\n",
      "Feature: word_unigram, Model: MLP, Accuracy: 0.7500, Precision: 0.8061, Recall: 0.7500, F1-score: 0.7483\n",
      "Feature: word_unigram, Model: Decision Tree, Accuracy: 0.3083, Precision: 0.3221, Recall: 0.3083, F1-score: 0.3008\n",
      "Feature: word_bigram_trigram, Model: Random Forest, Accuracy: 0.3625, Precision: 0.5756, Recall: 0.3625, F1-score: 0.3824\n",
      "Feature: word_bigram_trigram, Model: SVM, Accuracy: 0.0167, Precision: 0.0293, Recall: 0.0167, F1-score: 0.0076\n",
      "Feature: word_bigram_trigram, Model: XGBoost, Accuracy: 0.2333, Precision: 0.3728, Recall: 0.2333, F1-score: 0.2512\n",
      "Feature: word_bigram_trigram, Model: Naive Bayes, Accuracy: 0.2125, Precision: 0.3832, Recall: 0.2125, F1-score: 0.2169\n",
      "Feature: word_bigram_trigram, Model: MLP, Accuracy: 0.2625, Precision: 0.5405, Recall: 0.2625, F1-score: 0.2676\n",
      "Feature: word_bigram_trigram, Model: Decision Tree, Accuracy: 0.2208, Precision: 0.2672, Recall: 0.2208, F1-score: 0.2191\n",
      "Feature: char_bigram_trigram, Model: Random Forest, Accuracy: 0.8625, Precision: 0.8917, Recall: 0.8625, F1-score: 0.8523\n",
      "Feature: char_bigram_trigram, Model: SVM, Accuracy: 0.9000, Precision: 0.9213, Recall: 0.9000, F1-score: 0.9012\n",
      "Feature: char_bigram_trigram, Model: XGBoost, Accuracy: 0.8833, Precision: 0.8973, Recall: 0.8833, F1-score: 0.8802\n",
      "Feature: char_bigram_trigram, Model: Naive Bayes, Accuracy: 0.1000, Precision: 0.0862, Recall: 0.1000, F1-score: 0.0600\n",
      "Feature: char_bigram_trigram, Model: MLP, Accuracy: 0.9292, Precision: 0.9365, Recall: 0.9292, F1-score: 0.9280\n",
      "Feature: char_bigram_trigram, Model: Decision Tree, Accuracy: 0.6500, Precision: 0.6641, Recall: 0.6500, F1-score: 0.6469\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# TF-IDF based evaluations\n",
    "for vec_name, vectorizer in vectorizer_settings.items():\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Evaluating {model_name} with {vec_name}...\")\n",
    "        pipeline = Pipeline([(\"tfidf\", vectorizer), (\"clf\", model)])\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        report = classification_report(\n",
    "            y_test, y_pred, output_dict=True, zero_division=0\n",
    "        )\n",
    "        results.append(\n",
    "            {\n",
    "                \"Feature\": vec_name,\n",
    "                \"Model\": model_name,\n",
    "                \"Accuracy\": report[\"accuracy\"],\n",
    "                \"Precision\": report[\"weighted avg\"][\"precision\"],\n",
    "                \"Recall\": report[\"weighted avg\"][\"recall\"],\n",
    "                \"F1-score\": report[\"weighted avg\"][\"f1-score\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(\"TF-IDF Results:\")\n",
    "for result in results:\n",
    "    print(\n",
    "        f\"Feature: {result['Feature']}, Model: {result['Model']}, \"\n",
    "        f\"Accuracy: {result['Accuracy']:.4f}, Precision: {result['Precision']:.4f}, \"\n",
    "        f\"Recall: {result['Recall']:.4f}, F1-score: {result['F1-score']:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8b05eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name sentence-transformers/all-MiniLM-L6-v2. Creating a new one with mean pooling.\n",
      "Batches: 100%|██████████| 30/30 [00:22<00:00,  1.32it/s]\n",
      "Batches: 100%|██████████| 8/8 [00:05<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Random Forest with BERT embeddings...\n",
      "Evaluating SVM with BERT embeddings...\n",
      "Evaluating XGBoost with BERT embeddings...\n",
      "Evaluating MLP with BERT embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/osmanyalcin/Desktop/ödevler/data mining/bert_env/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Decision Tree with BERT embeddings...\n",
      "BERT Results:\n",
      "Feature: BERT, Model: Random Forest, Accuracy: 0.1708, Precision: 0.1815, Recall: 0.1708, F1-score: 0.1531\n",
      "Feature: BERT, Model: SVM, Accuracy: 0.1708, Precision: 0.1986, Recall: 0.1708, F1-score: 0.1470\n",
      "Feature: BERT, Model: XGBoost, Accuracy: 0.2292, Precision: 0.2412, Recall: 0.2292, F1-score: 0.2280\n",
      "Feature: BERT, Model: MLP, Accuracy: 0.4042, Precision: 0.4310, Recall: 0.4042, F1-score: 0.4101\n",
      "Feature: BERT, Model: Decision Tree, Accuracy: 0.1000, Precision: 0.1391, Recall: 0.1000, F1-score: 0.1001\n",
      "Results saved to results.csv\n",
      "Results saved to results_bert.csv\n"
     ]
    }
   ],
   "source": [
    "# BERT embeddings\n",
    "bert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "X_train_bert = bert_model.encode(X_train.tolist(), show_progress_bar=True)\n",
    "X_test_bert = bert_model.encode(X_test.tolist(), show_progress_bar=True)\n",
    "\n",
    "bert_compatible_models = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss'),\n",
    "    \"MLP\": MLPClassifier(max_iter=300),\n",
    "    \"Decision Tree\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "# TMP\n",
    "results_bert = []\n",
    "\n",
    "for model_name, model in bert_compatible_models.items(): \n",
    "    print(f\"Evaluating {model_name} with BERT embeddings...\")\n",
    "    model.fit(X_train_bert, y_train)\n",
    "    y_pred = model.predict(X_test_bert)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "    results_bert.append(\n",
    "        {\n",
    "            \"Feature\": \"BERT\",\n",
    "            \"Model\": model_name,\n",
    "            \"Accuracy\": report[\"accuracy\"],\n",
    "            \"Precision\": report[\"weighted avg\"][\"precision\"],\n",
    "            \"Recall\": report[\"weighted avg\"][\"recall\"],\n",
    "            \"F1-score\": report[\"weighted avg\"][\"f1-score\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"BERT Results:\")\n",
    "for result in results_bert:\n",
    "    if result[\"Feature\"] == \"BERT\":\n",
    "        print(\n",
    "            f\"Feature: {result['Feature']}, Model: {result['Model']}, \"\n",
    "            f\"Accuracy: {result['Accuracy']:.4f}, Precision: {result['Precision']:.4f}, \"\n",
    "            f\"Recall: {result['Recall']:.4f}, F1-score: {result['F1-score']:.4f}\"\n",
    "        )\n",
    "# Save results to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"results.csv\", index=False)\n",
    "print(\"Results saved to results.csv\")\n",
    "results_bert_df = pd.DataFrame(results_bert)\n",
    "results_bert_df.to_csv(\"results_bert.csv\", index=False)\n",
    "print(\"Results saved to results_bert.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70df3307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All results saved to results_all.csv\n"
     ]
    }
   ],
   "source": [
    "result_all = pd.concat([results_df, results_bert_df], ignore_index=True)\n",
    "result_all.to_csv(\"results_all.csv\", index=False)\n",
    "print(\"All results saved to results_all.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
